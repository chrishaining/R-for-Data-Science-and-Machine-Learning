opbABTop2
# it would be useful to take a look at salaries again.
combo2001 %>% select(playerID, AB, OBP, salary) %>% arrange(desc(AB))
# this shows that the top three players by AB would cost a combined
costABTop <- 5666667 + 4500000 + 200000
costABTop # 10,366,667 - well under 15 million, so let's see what the OBP would be
opbABTop <- (0.3814714	+ 0.3139205 + 0.3234880)/3
opbABTop # 0.3396266 - this is less than the requirement of 0.3638687, so we need to raise this. We have some leeway, as the top 10 players by AB all have scores that would mean the combined AB was sufficient.
#replace the lowest OBP
opbABTop2 <- (0.3814714	+ 0.3710602 + 0.3688047)/3 # this results in a mean of 0.3737788
opbABTop2 # suzukic01 + stewash01 + 0.3688047
692 + 640 + 636
# it would be useful to take a look at salaries again.
combo2001 %>% select(playerID, AB, OBP, salary) %>% arrange(desc(AB))
# this shows that the top three players by AB would cost a combined
costABTop <- 5666667 + 4500000 + 200000
costABTop # 10,366,667 - well under 15 million, so let's see what the OBP would be
opbABTop <- (0.3814714	+ 0.3139205 + 0.3234880)/3
opbABTop # 0.3396266 - this is less than the requirement of 0.3638687, so we need to raise this. We have some leeway, as the top 10 players by AB all have scores that would mean the combined AB was sufficient.
#replace the lowest OBP
opbABTop2 <- (0.3814714	+ 0.3710602 + 0.3688047)/3 # this results in a mean of 0.3737788
opbABTop2 # suzukic01 + stewash01 + 0.3688047
#AB = 692 + 640 + 636 = 1968, so that meets the requirement
5666667 + 2183333 + 3250000
# it would be useful to take a look at salaries again.
combo2001 %>% select(playerID, AB, OBP, salary) %>% arrange(desc(AB))
# this shows that the top three players by AB would cost a combined
costABTop <- 5666667 + 4500000 + 200000
costABTop # 10,366,667 - well under 15 million, so let's see what the OBP would be
opbABTop <- (0.3814714	+ 0.3139205 + 0.3234880)/3
opbABTop # 0.3396266 - this is less than the requirement of 0.3638687, so we need to raise this. We have some leeway, as the top 10 players by AB all have scores that would mean the combined AB was sufficient.
#replace the lowest OBP
opbABTop2 <- (0.3814714	+ 0.3710602 + 0.3688047)/3 # this results in a mean of 0.3737788
opbABTop2 # suzukic01 + stewash01 + 0.3688047
#AB = 692 + 640 + 636 = 1968, so that meets the requirement
#combined salary 5666667 + 2183333 + 3250000 = 11,100,000 - so this meets the requirement
ids <- c('suzukic01' + 'stewash01' + 'aurilri01')
ids <- c('suzukic01','stewash01','aurilri01')
replacements <- subset(combo2001, playerID %in% ids)
replacements
ids <- c('suzukic01','stewash01','aurilri01')
replacements <- subset(combo2001, playerID %in% ids)
replacements
totalAB <- replacements %>% summarise(sum(AB))
meanOBP <- replacements %>% summarise(mean(OBP))
totalSalary <- replacements %>% summarise(sum(salary))
library(stringr)
str_glue("The total AB score is {totalAB}")
str_glue("The mean OBP score is {meanOBP}")
str_glue("The total salary is {totalSalary}")
ids <- c('suzukic01','stewash01','aurilri01')
replacements <- subset(combo2001, playerID %in% ids)
replacements
library(stringr)
totalAB <- replacements %>% summarise(sum(AB))
meanOBP <- replacements %>% summarise(mean(OBP))
totalSalary <- replacements %>% summarise(sum(salary))
str_glue("The total AB score is {totalAB}")
str_glue("The mean OBP score is {meanOBP}")
str_glue("The total salary is {totalSalary}")
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv')
head(df)
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv', sep = ')
head(df)
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv', sep = ';)
head(df)
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv', sep = ';)
head(df)
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv', sep = ';')
head(df)
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv', sep = ';')
head(df)
summary(df)
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv', sep = ';')
head(df)
summary(df)
any(is.na(df))
library(ggplot2)
library(ggthemes)
library(dplyr)
numCols <- sapply(df,is.numeric)
corData <- cor(df[numCols])
corData
install.packages(c("corrgram", "corrplot"))
library(corrgram)
install.packages("corrplot")
install.packages("corrplot")
install.packages("corrplot")
install.packages("corrgram")
library(corrgram)
install.packages('corrgram')
install.packages("corrgram")
install.packages(c("corrgram", "corrplot"))
install.packages("corrgram")
library(readr)
df <- read.csv('Machine Learning with R/student-mat.csv', sep = ';')
head(df)
summary(df)
any(is.na(df))
library(ggplot2)
library(ggthemes)
library(dplyr)
numCols <- sapply(df,is.numeric)
corData <- cor(df[numCols])
corData
library(corrgram)
install.packages("corrplot")
install.packages("corrgram")
library(corrgram)
library(corrplot)
library(corrgram)
library(corrplot)
corrplot(corData)
library(corrgram)
library(corrplot)
corrplot(corData,method='color')
corrgram(df)
corrgram(df,order=T,lower.panel = panel.pie, upper.panel = panel.bar())
corrgram(df,order=T,lower.panel = panel.pie, upper.panel = panel.shade)
corrgram(df,order=T,lower.panel = panel.pie, upper.panel = panel.shade, text.panel = panel.txt)
ggplot(df,aes(x=G3)) + geom_histogram(bins=20,alpha=0.5,fill='blue')
library(caTools)
summary(model)
# for a linear regression model
#model <- lm(y ~ x1 + x2,data)
# quicker way for a linear regression model
#model <- lm(y ~ . , data)
model <- lm(G3 ~ ., data = train)
library(caTools)
# set a seeds file of random data
set.seed(101)
# state the way you are splitting the data for the sample. arguments are a dataframe column and splitRatio (the proportion of the data you want to use for the sample)
sample <- sample.split(df$G3,SplitRatio = 0.7)
# create the training subset (i.e. here it's 0.7 of the data)
train <- subset(df, sample == TRUE)
# which means the test subset should be 0.3
test <- subset(df,sample==FALSE)
# for a linear regression model
#model <- lm(y ~ x1 + x2,data)
# quicker way for a linear regression model
#model <- lm(y ~ . , data)
model <- lm(G3 ~ ., data = train)
summary(model)
res <- residuals(model)
class(res)
res <- residuals(model)
class(res)
res <- as.data.frame(res)
res <- residuals(model)
class(res)
res <- as.data.frame(res)
ggplot(res,aes(res)) _ geom_histogram()
res <- residuals(model)
class(res)
res <- as.data.frame(res)
ggplot(res,aes(res)) + geom_histogram()
res <- residuals(model)
class(res)
res <- as.data.frame(res)
ggplot(res,aes(res)) + geom_histogram(binwidth=20)
res <- residuals(model)
class(res)
res <- as.data.frame(res)
ggplot(res,aes(res)) + geom_histogram(fill='blue')
res <- residuals(model)
class(res)
res <- as.data.frame(res)
ggplot(res,aes(res)) + geom_histogram(fill='blue', alpha=0.5)
plot(model)
G3Predictions <- predict(model, test)
G3Predictions <- predict(model, test)
results <- cbind(G3Predictions, test$G3)
colnames(results) <- c('pred','real')
results <- as.data.frame(results)
G3Predictions <- predict(model, test)
results <- cbind(G3Predictions, test$G3)
colnames(results) <- c('pred','real')
results <- as.data.frame(results)
head(results)
toZero <- function(x) {
if (x <0) {return(0)} else {return(x)}
}
toZero <- function(x) {
if (x <0) {return(0)} else {return(x)}
}
# 1. use Mean Standard Error
mse <- mean((results$real-results%pred)^2)
# 1. use Mean Standard Error
mse <- mean((results$real-results$pred)^2)
mse
# 2. Root Mean Squared Error
mse^0.5
# 3. R-squared value
SSE <- sum((results$pred-results$real)^2)
SST <- sum(mean(df$G3)-results$real)^2)
# 3. R-squared value
SSE <- sum((results$pred-results$real)^2)
SST <- sum((mean(df$G3)-results$real)^2)
R2 <- 1- SSE/SST
R2
G3Predictions <- predict(model, test)
results <- cbind(G3Predictions, test$G3)
colnames(results) <- c('pred','real')
results <- as.data.frame(results)
head(results)
min(results)
toZero <- function(x) {
if (x <0) {return(0)} else {return(x)}
}
results$pred <- sapply(results$pred,toZero)
min(results)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
summary(bike)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
corr(bike)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
cor(bike)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
bikeNums <- any(is.numeric(bike))
cor(bikeNums)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
bikeNums <- any(bike, is.numeric
cor(bikeNums)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
bikeNums <- any(bike, is.numeric)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
numBike <- sapply(bike,is.numeric)
cor(numBike)
library(readr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
numBike <- sapply(bike,is.numeric)
cor(bike[numBike])
library(readr)
library(ggplot2)
library(plotly)
library(ggthemes)
library(dplyr)
bike <- read.csv('Training Exercises/Machine Learning Projects/CSV files for ML Projects/bikeshare.csv')
head(bike)
tail(bike)
summary(bike)
numBike <- sapply(bike,is.numeric)
cor(bike[numBike])
ggplot(bike, aes(x=temp, y=count)) + geom_point(alpha=0.5)
ggplot(bike, aes(x=temp, y=count, fill=temp)) + geom_point(alpha=0.5)
ggplot(bike, aes(x=temp, y=count, fill=temp)) + geom_point(alpha=0.4)
bike <- bike %>% mutate(datetime=as.POSIXct(datetime))
head(bike)
bike <- bike %>% mutate(datetime=as.POSIXct(datetime))
#head(bike)
ggplot(bike, aes(x=datetime, y=count)) + geom_point(alpha=0.4)
bike <- bike %>% mutate(datetime=as.POSIXct(datetime))
#head(bike)
ggplot(bike, aes(x=datetime, y=count, fill=temp)) + geom_point(alpha=0.4)
bike <- bike %>% mutate(datetime=as.POSIXct(datetime))
#head(bike)
ggplot(bike, aes(x=datetime, y=count)) + geom_point(alpha=0.4, fill=temp)
bike <- bike %>% mutate(datetime=as.POSIXct(datetime))
#head(bike)
ggplot(bike, aes(x=datetime, y=count)) + geom_point(aes(alpha=0.4, fill=temp))
bike <- bike %>% mutate(datetime=as.POSIXct(datetime))
#head(bike)
ggplot(bike, aes(x=datetime, y=count, color=temp)) + geom_point(alpha=0.4)
ggplot(bike, aes(x=temp, y=count, color=temp)) + geom_point(alpha=0.4)
cor(bike$temp, bike$count)
ggplot(bike, aes(x=season, y=count)) + geom_boxplot()
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue')
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue') + theme_linedraw()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue') + theme_linedraw() + coord_flip()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue') + theme_linedraw() + coord_flip() + geom_text(aes(label=count))
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue') + theme_linedraw() + coord_flip() + geom_text(label=count)
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue') + theme_linedraw() + coord_flip() + geom_text()
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue') + theme_linedraw() + coord_flip() + geom_text(label=count)
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue') + theme_linedraw() + coord_flip() + geom_text(aes(label=count))
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue',alpha=0.5) + theme_linedraw() + coord_flip()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill='blue',alpha=0.5) + theme_linedraw() + coord_flip()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(fill=factor(season),alpha=0.5) + theme_linedraw() + coord_flip()
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(aes(fill=factor(season),alpha=0.5)) + theme_linedraw() + coord_flip()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(aes(fill=factor(season)),alpha=0.5) + theme_linedraw() + coord_flip()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(aes(fill=factor(season)),alpha=0.5) + coord_flip()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(aes(fill=factor(season)),alpha=0.5) + coord_flip() + theme_clean()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(aes(fill=factor(season)),alpha=0.5) + coord_flip() + theme_solarized_2()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(aes(fill=factor(season)),alpha=0.5) + coord_flip() + theme_solarized()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
ggplot(bike, aes(x=factor(season), y=count)) + geom_boxplot(aes(fill=factor(season)),alpha=0.5) + coord_flip() + theme_classic()
# use coord_flip to flip the axes
# carBox <- ggplot(carData,aes(x=factor(cyl),y=mpg)) + geom_boxplot(aes(fill=factor(cyl))) + coord_flip() + theme_linedraw()
# carBox
bike <- bike %>% mutate(hour=format(datetime, "%H"))
head(bike)
bike2 <- bike %>% filter(workingday==1)
ggplot(bike2, aes(x=hour, y=count,color=temp)) + geom_point()
bike2 <- bike %>% filter(workingday==1)
ggplot(bike2, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4)
bike3 <- bike %>% filter(workingday==0)
ggplot(bike3, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4)
bike2 <- bike %>% filter(workingday==1)
ggplot(bike2, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4, position = position_jitter(w=1,h=0))
bike2 <- bike %>% filter(workingday==1)
myColors <- c('red', 'green', 'yellow', 'pink', 'blue')
ggplot(bike2, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4, position = position_jitter(w=1,h=0)) + scale_color_gradient(myColors)
bike2 <- bike %>% filter(workingday==1)
myColors <- c('red', 'green', 'yellow', 'pink', 'blue')
ggplot(bike2, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4, position = position_jitter(w=1,h=0)) + scale_color_gradient(colors=myColors)
bike2 <- bike %>% filter(workingday==1)
myColors <- c('red', 'green', 'yellow', 'pink', 'blue')
ggplot(bike2, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4, position = position_jitter(w=1,h=0)) + scale_color_gradientn(colors=myColors)
bike3 <- bike %>% filter(workingday==0)
ggplot(bike3, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4) + scale_color_gradientn(colors=myColors)
bike3 <- bike %>% filter(workingday==0)
ggplot(bike3, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4) + scale_color_gradientn(colors=myColors) + position_jitter(w=1,h=0))
bike3 <- bike %>% filter(workingday==0)
ggplot(bike3, aes(x=hour, y=count,color=temp)) + geom_point(alpha=0.4, position=position_jitter(w=1,h=0)) + scale_color_gradientn(colors=myColors)
tempModel <- lm(temp ~ ., data=bike)
summary(tempModel)
tempModel <- lm(count ~ temp, data=bike)
summary(tempModel)
myWay <- 25 * 9.1705
myWay
highWay <- predict(tempModel, bike)
highWay
myWay <- 6.0462*(25 * 9.1705)
myWay
highWay <- predict(tempModel, bike)
highWay
myWay <- 6.0462+(25 * 9.1705)
myWay
highWay <- predict(tempModel, bike)
highWay
myWay <- 6.0462+(25 * 9.1705)
myWay
temp.test <- data.frame(temp=c(25))
highWay <- predict(tempModel, temp.test)
highWay
bike$hour <- sapply(bike$hour,is.numeric)
head(bike)
bike$hour <- sapply(bike$hour,as.numeric)
head(bike)
model <- lm(count ~ . -casual - registered -datetime -atemp,bike )
summary(model)
library(readr)
library(dplyr)
dfTrain <- read.csv('Machine Learning with R/titanic_train.csv')
head(dfTrain)
install.packages("Amelia")
# import the Amelia package
library(Ameli)
# import the Amelia package
library(Amelia)
install.packages("Amelia")
install.packages("Amelia")
library(Amelia)
# import the Amelia package
library(Amelia)
# import the Amelia package
library(Amelia)
missmap(df.train, main="Titanic Training Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
# import the Amelia package
library(Amelia)
missmap(dfTrain, main="Titanic Training Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
library(ggplot2)
ggplot(dfTrain, aes(Survived)) + geom_bar()
ggplot(dfTrain,aes(Pclass)) + geom_bar(aes(fill=factor(Pclass)),alpha=0.5)
ggplot(dfTrain,aes(Pclass)) + geom_bar(aes(fill=factor(Pclass)),alpha=0.5) + theme_clean()
ggplot(dfTrain,aes(Pclass)) + geom_bar(aes(fill=factor(Pclass)),alpha=0.5) + theme_base()
ggplot(dfTrain,aes(Sex)) + geom_bar(aes(fill=factor(Sex)),alpha=0.5) + theme_base()
nrow(dfTrain)
# ggplot(dfTrain,aes(Age)) + geom_histogram(fill='blue',bins=20,alpha=0.5)
nrow(dfTrain)
0.2*891
# ggplot(dfTrain,aes(Age)) + geom_histogram(fill='blue',bins=20,alpha=0.5)
ggplot(dfTrain,aes(Age)) + geom_histogram(fill='blue',bins=20,alpha=0.5)
ggplot(dfTrain,aes(SibSp)) + geom_bar(fill='red',alpha=0.5)
ggplot(dfTrain,aes(Fare)) + geom_histogram(fill='green',color='black',alpha=0.5)
pl <- ggplot(dfTrain,aes(Pclass,Age)) + geom_boxplot(aes(group=Pclass,fill=factor(Pclass),alpha=0.4))
pl + scale_y_continuous(breaks = seq(min(0), max(80), by = 2))
fixedAges <- impute_age(dfTrain$Age,dfTrain$Pclass)
impute_age <- function(age,class){
out <- age
for (i in 1:length(age)){
if (is.na(age[i])){
if (class[i] == 1){
out[i] <- 37
}else if (class[i] == 2){
out[i] <- 29
}else{
out[i] <- 24
}
}else{
out[i]<-age[i]
}
}
return(out)
}
fixedAges <- impute_age(dfTrain$Age,dfTrain$Pclass)
dfTrain$Age <- fixedAges
missmap(df.train, main="Titanic Training Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
missmap(dfTrain, main="Titanic Training Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
str(dfTrain)
dfTrain <- select(dfTrain,-PassengerId,-Name,-Ticket,-Cabin)
str(dfTrain)
dfTrain$Survived <- factor(dfTrain$Survived)
dfTrain$Pclass <- factor(dfTrain$Pclass)
dfTrain$Parch <- factor(dfTrain$Parch)
dfTrain$SibSp <- factor(dfTrain$SibSp)
str(dfTrain)
logModel <- glm(formula=Survived ~ . , family = binomial(link='logit'),data = dfTrain)
summary(logModel)
library(caTools)
set.seed(101)
split = sample.split(dfTrain$Survived, SplitRatio = 0.70)
finalTrain = subset(dfTrain, split == TRUE)
finalTest = subset(dfTrain, split == FALSE)
finalLogModel <- glm(formula=Survived ~ . , family = binomial(link='logit'),data = finalTrain)
summary(finalLogModel)
fittedProbabilities <- predict(finalLogModel,newdata=finalTest,type='response')
fittedProbabilities <- predict(finalLogModel,newdata=finalTest,type='response')
fittedResults <- ifelse(fittedProbabilities > 0.5,1,0)
misClasificError <- mean(fittedResults != finalTest$Survived)
print(paste('Accuracy',1-misClasificError))
table(final.test$Survived, fitted.probabilities > 0.5)
table(finalTest$Survived, fittedProbabilities > 0.5)
confusionMatrix <- table(finalTest$Survived, fittedProbabilities > 0.5)
confusionMatrix
class(confusionMatrix)
