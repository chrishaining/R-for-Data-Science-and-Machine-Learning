---
title: "Neural Networks"
output: html_notebook
---
 

##Â get data 
The data describes housing in Boston, Massachussetts in 1978.
```{r}

library(MASS)
head(Boston)

```
 
## check how big the dataframe is
```{r}

str(Boston)

```
There are 506 observations

## check for missing data
```{r}

any(is.na(Boston))

```
There are no missing data.


## assign the dataset to a variable 
This allows us to mess around with the data.
```{r}

data <- Boston

```


## normalise the data
__Why are we doing this?__
Normalising the data improves the efficiency of the learning.
[towardsdatascience](https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d)

first, get the maximum values for each column
```{r}

maxes <- apply(data, MARGIN = 2, max)
maxes
```

second, get the minimum values for each column
```{r}

mins <- apply(data, MARGIN = 2, min)
mins

```

third, normalise the data
```{r}

# scale the data - creates a matrix
scaled.data <- scale(data, center = mins, scale = maxes-mins)

# turn the scaled.data into a dataframe (it was a matrix)
scaled.data <- as.data.frame(scaled.data)

head(scaled.data)

```


## split the data into training and test sets
```{r}
library(caTools)

split <- sample.split(scaled.data$medv, SplitRatio = 0.7)
train <- subset(scaled.data, split==T)
test <- subset(scaled.data, split==F)
head(train)
```


## train the model
The neuralnet library has a quirk whereby it does not allow the normal `y ~ .` format. Instead, you'll have to write all the column names individually. However, there is a workaround.
```{r}

library(neuralnet)

# workaround part 1 - get the names of the columns
n <- names(train)

# workaround part 2 - Paste together
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))

```

## train the data
```{r}
nn <- neuralnet(f,data=train,hidden=c(5,3),linear.output=TRUE)
plot(nn)
```

## make predictions
```{r}

predicted.nn.values <- compute(nn, test[1:13])
str(predicted.nn.values)

```

## convert the predictions to non-scaled predictions
```{r}

true.predictions <- predicted.nn.values$net.result*(max(data$medv)-min(data$medv))+min(data$medv)

```


## convert the test data
```{r}

test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)

```

## check the mean squared error
```{r}

MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test)
MSE.nn

```

# visualize the error

## create dataframe of errors
```{r}

error.df <- data.frame(test.r, true.predictions)
head(error.df)

```

## 
```{r}

library(ggplot2)

ggplot(error.df, aes(x=test.r, y=true.predictions)) + geom_point() + stat_smooth()

```

